{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd  \n",
    "import numpy as np    \n",
    "DATASET_FOLDER = '../dataset/'\n",
    "train = pd.read_csv('train.csv')\n",
    "test = pd.read_csv('test.csv')\n",
    "sample_test = pd.read_csv( 'sample_test.csv')\n",
    "sample_test_out = pd.read_csv('sample_test_out.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "val = train[20000:30000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image_link</th>\n",
       "      <th>group_id</th>\n",
       "      <th>entity_name</th>\n",
       "      <th>entity_value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>20000</th>\n",
       "      <td>https://m.media-amazon.com/images/I/717DLM3B5d...</td>\n",
       "      <td>529606</td>\n",
       "      <td>item_weight</td>\n",
       "      <td>7.0 ounce</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20001</th>\n",
       "      <td>https://m.media-amazon.com/images/I/71by89Xk-p...</td>\n",
       "      <td>299791</td>\n",
       "      <td>item_weight</td>\n",
       "      <td>112.0 gram</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20002</th>\n",
       "      <td>https://m.media-amazon.com/images/I/61ud3N26zA...</td>\n",
       "      <td>179080</td>\n",
       "      <td>voltage</td>\n",
       "      <td>48.0 volt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20003</th>\n",
       "      <td>https://m.media-amazon.com/images/I/51qmCZ9jTT...</td>\n",
       "      <td>254046</td>\n",
       "      <td>item_weight</td>\n",
       "      <td>0.128 kilogram</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20004</th>\n",
       "      <td>https://m.media-amazon.com/images/I/81xKX8h5-M...</td>\n",
       "      <td>483370</td>\n",
       "      <td>item_weight</td>\n",
       "      <td>1.0 pound</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29995</th>\n",
       "      <td>https://m.media-amazon.com/images/I/71PlsJl-mJ...</td>\n",
       "      <td>449021</td>\n",
       "      <td>item_weight</td>\n",
       "      <td>10.0 ounce</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29996</th>\n",
       "      <td>https://m.media-amazon.com/images/I/71o33-+F8e...</td>\n",
       "      <td>522832</td>\n",
       "      <td>item_weight</td>\n",
       "      <td>17.99 ounce</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29997</th>\n",
       "      <td>https://m.media-amazon.com/images/I/61tBLvnkHo...</td>\n",
       "      <td>267482</td>\n",
       "      <td>item_weight</td>\n",
       "      <td>3.7 pound</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29998</th>\n",
       "      <td>https://m.media-amazon.com/images/I/71Qr7HYWoB...</td>\n",
       "      <td>267482</td>\n",
       "      <td>item_weight</td>\n",
       "      <td>3.7 pound</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29999</th>\n",
       "      <td>https://m.media-amazon.com/images/I/71YYBsIxgb...</td>\n",
       "      <td>267482</td>\n",
       "      <td>item_weight</td>\n",
       "      <td>3.7 pound</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10000 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              image_link  group_id  \\\n",
       "20000  https://m.media-amazon.com/images/I/717DLM3B5d...    529606   \n",
       "20001  https://m.media-amazon.com/images/I/71by89Xk-p...    299791   \n",
       "20002  https://m.media-amazon.com/images/I/61ud3N26zA...    179080   \n",
       "20003  https://m.media-amazon.com/images/I/51qmCZ9jTT...    254046   \n",
       "20004  https://m.media-amazon.com/images/I/81xKX8h5-M...    483370   \n",
       "...                                                  ...       ...   \n",
       "29995  https://m.media-amazon.com/images/I/71PlsJl-mJ...    449021   \n",
       "29996  https://m.media-amazon.com/images/I/71o33-+F8e...    522832   \n",
       "29997  https://m.media-amazon.com/images/I/61tBLvnkHo...    267482   \n",
       "29998  https://m.media-amazon.com/images/I/71Qr7HYWoB...    267482   \n",
       "29999  https://m.media-amazon.com/images/I/71YYBsIxgb...    267482   \n",
       "\n",
       "       entity_name    entity_value  \n",
       "20000  item_weight       7.0 ounce  \n",
       "20001  item_weight      112.0 gram  \n",
       "20002      voltage       48.0 volt  \n",
       "20003  item_weight  0.128 kilogram  \n",
       "20004  item_weight       1.0 pound  \n",
       "...            ...             ...  \n",
       "29995  item_weight      10.0 ounce  \n",
       "29996  item_weight     17.99 ounce  \n",
       "29997  item_weight       3.7 pound  \n",
       "29998  item_weight       3.7 pound  \n",
       "29999  item_weight       3.7 pound  \n",
       "\n",
       "[10000 rows x 4 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "xval = val[\"image_link\"].values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                            | 16/10000 [01:24<12:46:01,  4.60s/it]"
     ]
    }
   ],
   "source": [
    "#from utils import download_images\n",
    "#download_images(xval, \"images_6\"  , allow_multiprocessing=True)\n",
    "#only 9858 downloaded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import easyocr\n",
    "# import os\n",
    "# import pandas as pd\n",
    "# import re\n",
    "# import cv2\n",
    "\n",
    "# # Initialize the EasyOCR Reader to use GPU (gpu=True)\n",
    "# reader = easyocr.Reader(['en'], gpu=True)\n",
    "\n",
    "# # Function to correct OCR misinterpretations based on context\n",
    "# def correct_ocr_text(text):\n",
    "#     # Replaces 'O' with '0' if it appears in a numerical context (surrounded by digits)\n",
    "    \n",
    "#     words = text.split()\n",
    "#     corrected_words = []\n",
    "#     for word in words:\n",
    "#         # If the word contains digits, correct 'O' or 'o' to '0'\n",
    "#         if re.search(r'\\d', word):\n",
    "#             corrected_word = re.sub(r'[Oo]', '0', word)\n",
    "#         else:\n",
    "#             corrected_word = word\n",
    "        \n",
    "#         corrected_words.append(corrected_word)\n",
    "\n",
    "#     return ' '.join(corrected_words)\n",
    "\n",
    "# # Function to process images and save results to CSV in batches\n",
    "# def process_images_in_folder(folder_path, csv_output_file, batch_size=100):\n",
    "#     results = []\n",
    "    \n",
    "#     # Get all image file paths in the folder\n",
    "#     image_files = [f for f in os.listdir(folder_path) if f.endswith(('.png', '.jpg', '.jpeg'))]\n",
    "    \n",
    "#     total_images = len(image_files)\n",
    "#     print(f\"Total images to process: {total_images}\")\n",
    "    \n",
    "#     for idx, image_file in enumerate(image_files):\n",
    "#         try:\n",
    "#             # Load the grayscale image from the folder\n",
    "#             image_path = os.path.join(folder_path, image_file)\n",
    "#             img = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)\n",
    "            \n",
    "#             if img is None:\n",
    "#                 print(f\"Error reading image {image_file}\")\n",
    "#                 continue\n",
    "\n",
    "#             # Use EasyOCR to extract text\n",
    "#             result = reader.readtext(img)\n",
    "            \n",
    "#             # Extract and concatenate text from the OCR results\n",
    "#             extracted_text = ' '.join([item[1] for item in result])\n",
    "\n",
    "#             # Correct OCR errors\n",
    "#             corrected_text = correct_ocr_text(extracted_text)\n",
    "            \n",
    "#             results.append({'image_file': image_file, 'extracted_text': corrected_text})\n",
    "            \n",
    "#             # Save to CSV after every batch_size images\n",
    "#             if (idx + 1) % batch_size == 0:\n",
    "#                 print(f\"Saving batch {idx + 1}/{total_images} to CSV...\")\n",
    "#                 results_df = pd.DataFrame(results, columns=['image_file', 'extracted_text'])\n",
    "#                 if os.path.exists(csv_output_file):\n",
    "#                     # Append without overwriting\n",
    "#                     results_df.to_csv(csv_output_file, mode='a', header=False, index=False)\n",
    "#                 else:\n",
    "#                     results_df.to_csv(csv_output_file, index=False)\n",
    "#                 results = []  # Clear the batch after saving\n",
    "\n",
    "#         except Exception as e:\n",
    "#             print(f\"Error processing {image_file}: {e}\")\n",
    "#             results.append({'image_file': image_file, 'extracted_text': '', 'error': str(e)})\n",
    "    \n",
    "#     # Save any remaining results after the loop\n",
    "#     if results:\n",
    "#         results_df = pd.DataFrame(results, columns=['image_file', 'extracted_text'])\n",
    "#         if os.path.exists(csv_output_file):\n",
    "#             results_df.to_csv(csv_output_file, mode='a', header=False, index=False)\n",
    "#         else:\n",
    "#             results_df.to_csv(csv_output_file, index=False)\n",
    "    \n",
    "#     print(\"Processing complete!\")\n",
    "\n",
    "# # Example usage: Process images from the folder on D drive and save to CSV\n",
    "# # folder_path = r'D:\\your_image_folder'\n",
    "# # # Replace with the actual path to your images folder on D drive\n",
    "# folder_path = r'images_4'\n",
    "# csv_output_file = \"train_image_4.csv\"\n",
    "\n",
    "# # Process images and save results in batches of 100 images\n",
    "# process_images_in_folder(folder_path, csv_output_file, batch_size=100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ashil\\AppData\\Roaming\\Python\\Python312\\site-packages\\easyocr\\detection.py:85: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  net.load_state_dict(copyStateDict(torch.load(trained_model, map_location=device)))\n",
      "C:\\Users\\ashil\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\_utils.py:89: UserWarning: expandable_segments not supported on this platform (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\c10/cuda/CUDAAllocatorConfig.h:28.)\n",
      "  untyped_storage = torch.UntypedStorage(self.size(), device=device)\n",
      "C:\\Users\\ashil\\AppData\\Roaming\\Python\\Python312\\site-packages\\easyocr\\recognition.py:182: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(model_path, map_location=device))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total images to process: 9870\n",
      "Saving batch 50/9870 to CSV...\n",
      "Saving batch 100/9870 to CSV...\n",
      "Saving batch 150/9870 to CSV...\n",
      "Saving batch 200/9870 to CSV...\n",
      "Saving batch 250/9870 to CSV...\n",
      "Saving batch 300/9870 to CSV...\n",
      "Saving batch 350/9870 to CSV...\n",
      "Saving batch 400/9870 to CSV...\n",
      "Saving batch 450/9870 to CSV...\n",
      "Saving batch 500/9870 to CSV...\n",
      "Saving batch 550/9870 to CSV...\n",
      "Saving batch 600/9870 to CSV...\n",
      "Saving batch 650/9870 to CSV...\n",
      "Saving batch 700/9870 to CSV...\n",
      "Saving batch 750/9870 to CSV...\n",
      "Saving batch 800/9870 to CSV...\n",
      "Saving batch 850/9870 to CSV...\n",
      "Saving batch 900/9870 to CSV...\n",
      "Saving batch 950/9870 to CSV...\n",
      "Saving batch 1000/9870 to CSV...\n",
      "Saving batch 1050/9870 to CSV...\n",
      "Saving batch 1100/9870 to CSV...\n",
      "Saving batch 1150/9870 to CSV...\n",
      "Saving batch 1200/9870 to CSV...\n",
      "Saving batch 1250/9870 to CSV...\n",
      "Saving batch 1300/9870 to CSV...\n",
      "Saving batch 1350/9870 to CSV...\n",
      "Saving batch 1400/9870 to CSV...\n",
      "Saving batch 1450/9870 to CSV...\n",
      "Saving batch 1500/9870 to CSV...\n",
      "Saving batch 1550/9870 to CSV...\n",
      "Saving batch 1600/9870 to CSV...\n",
      "Saving batch 1650/9870 to CSV...\n",
      "Saving batch 1700/9870 to CSV...\n",
      "Saving batch 1750/9870 to CSV...\n",
      "Saving batch 1800/9870 to CSV...\n",
      "Saving batch 1850/9870 to CSV...\n",
      "Saving batch 1900/9870 to CSV...\n",
      "Saving batch 1950/9870 to CSV...\n",
      "Saving batch 2000/9870 to CSV...\n",
      "Saving batch 2050/9870 to CSV...\n",
      "Saving batch 2100/9870 to CSV...\n",
      "Saving batch 2150/9870 to CSV...\n",
      "Saving batch 2200/9870 to CSV...\n",
      "Saving batch 2250/9870 to CSV...\n",
      "Saving batch 2300/9870 to CSV...\n",
      "Saving batch 2350/9870 to CSV...\n",
      "Saving batch 2400/9870 to CSV...\n",
      "Saving batch 2450/9870 to CSV...\n",
      "Saving batch 2500/9870 to CSV...\n",
      "Saving batch 2550/9870 to CSV...\n",
      "Saving batch 2600/9870 to CSV...\n",
      "Saving batch 2650/9870 to CSV...\n",
      "Saving batch 2700/9870 to CSV...\n",
      "Saving batch 2750/9870 to CSV...\n",
      "Saving batch 2800/9870 to CSV...\n",
      "Saving batch 2850/9870 to CSV...\n",
      "Saving batch 2900/9870 to CSV...\n",
      "Saving batch 2950/9870 to CSV...\n",
      "Saving batch 3000/9870 to CSV...\n",
      "Saving batch 3050/9870 to CSV...\n",
      "Saving batch 3100/9870 to CSV...\n",
      "Saving batch 3150/9870 to CSV...\n",
      "Saving batch 3200/9870 to CSV...\n",
      "Saving batch 3250/9870 to CSV...\n",
      "Saving batch 3300/9870 to CSV...\n",
      "Saving batch 3350/9870 to CSV...\n",
      "Saving batch 3400/9870 to CSV...\n",
      "Saving batch 3450/9870 to CSV...\n",
      "Saving batch 3500/9870 to CSV...\n",
      "Saving batch 3550/9870 to CSV...\n",
      "Saving batch 3600/9870 to CSV...\n",
      "Saving batch 3650/9870 to CSV...\n",
      "Saving batch 3700/9870 to CSV...\n",
      "Saving batch 3750/9870 to CSV...\n",
      "Saving batch 3800/9870 to CSV...\n",
      "Saving batch 3850/9870 to CSV...\n",
      "Saving batch 3900/9870 to CSV...\n",
      "Saving batch 3950/9870 to CSV...\n",
      "Saving batch 4000/9870 to CSV...\n",
      "Saving batch 4050/9870 to CSV...\n",
      "Saving batch 4100/9870 to CSV...\n",
      "Saving batch 4150/9870 to CSV...\n",
      "Saving batch 4200/9870 to CSV...\n",
      "Saving batch 4250/9870 to CSV...\n",
      "Saving batch 4300/9870 to CSV...\n",
      "Saving batch 4350/9870 to CSV...\n",
      "Saving batch 4400/9870 to CSV...\n",
      "Saving batch 4450/9870 to CSV...\n",
      "Saving batch 4500/9870 to CSV...\n",
      "Saving batch 4550/9870 to CSV...\n",
      "Saving batch 4600/9870 to CSV...\n",
      "Saving batch 4650/9870 to CSV...\n",
      "Saving batch 4700/9870 to CSV...\n",
      "Saving batch 4750/9870 to CSV...\n",
      "Saving batch 4800/9870 to CSV...\n",
      "Saving batch 4850/9870 to CSV...\n",
      "Saving batch 4900/9870 to CSV...\n",
      "Saving batch 4950/9870 to CSV...\n",
      "Saving batch 5000/9870 to CSV...\n",
      "Saving batch 5050/9870 to CSV...\n",
      "Saving batch 5100/9870 to CSV...\n",
      "Saving batch 5150/9870 to CSV...\n",
      "Saving batch 5200/9870 to CSV...\n",
      "Saving batch 5250/9870 to CSV...\n",
      "Saving batch 5300/9870 to CSV...\n",
      "Saving batch 5350/9870 to CSV...\n",
      "Saving batch 5400/9870 to CSV...\n",
      "Saving batch 5450/9870 to CSV...\n",
      "Saving batch 5500/9870 to CSV...\n",
      "Saving batch 5550/9870 to CSV...\n",
      "Saving batch 5600/9870 to CSV...\n",
      "Saving batch 5650/9870 to CSV...\n",
      "Saving batch 5700/9870 to CSV...\n",
      "Saving batch 5750/9870 to CSV...\n",
      "Saving batch 5800/9870 to CSV...\n",
      "Saving batch 5850/9870 to CSV...\n",
      "Saving batch 5900/9870 to CSV...\n",
      "Saving batch 5950/9870 to CSV...\n",
      "Saving batch 6000/9870 to CSV...\n",
      "Saving batch 6050/9870 to CSV...\n",
      "Saving batch 6100/9870 to CSV...\n",
      "Saving batch 6150/9870 to CSV...\n",
      "Saving batch 6200/9870 to CSV...\n",
      "Saving batch 6250/9870 to CSV...\n",
      "Saving batch 6300/9870 to CSV...\n",
      "Saving batch 6350/9870 to CSV...\n",
      "Saving batch 6400/9870 to CSV...\n",
      "Saving batch 6450/9870 to CSV...\n",
      "Saving batch 6500/9870 to CSV...\n",
      "Saving batch 6550/9870 to CSV...\n",
      "Saving batch 6600/9870 to CSV...\n",
      "Saving batch 6650/9870 to CSV...\n",
      "Saving batch 6700/9870 to CSV...\n",
      "Saving batch 6750/9870 to CSV...\n",
      "Saving batch 6800/9870 to CSV...\n",
      "Saving batch 6850/9870 to CSV...\n",
      "Saving batch 6900/9870 to CSV...\n",
      "Saving batch 6950/9870 to CSV...\n",
      "Saving batch 7000/9870 to CSV...\n",
      "Saving batch 7050/9870 to CSV...\n",
      "Saving batch 7100/9870 to CSV...\n",
      "Saving batch 7150/9870 to CSV...\n",
      "Saving batch 7200/9870 to CSV...\n",
      "Saving batch 7250/9870 to CSV...\n",
      "Saving batch 7300/9870 to CSV...\n",
      "Saving batch 7350/9870 to CSV...\n",
      "Saving batch 7400/9870 to CSV...\n",
      "Saving batch 7450/9870 to CSV...\n",
      "Saving batch 7500/9870 to CSV...\n",
      "Saving batch 7550/9870 to CSV...\n",
      "Saving batch 7600/9870 to CSV...\n",
      "Saving batch 7650/9870 to CSV...\n",
      "Saving batch 7700/9870 to CSV...\n",
      "Saving batch 7750/9870 to CSV...\n",
      "Saving batch 7800/9870 to CSV...\n",
      "Saving batch 7850/9870 to CSV...\n",
      "Saving batch 7900/9870 to CSV...\n",
      "Saving batch 7950/9870 to CSV...\n",
      "Saving batch 8000/9870 to CSV...\n",
      "Saving batch 8050/9870 to CSV...\n",
      "Saving batch 8100/9870 to CSV...\n",
      "Saving batch 8150/9870 to CSV...\n",
      "Saving batch 8200/9870 to CSV...\n",
      "Saving batch 8250/9870 to CSV...\n",
      "Saving batch 8300/9870 to CSV...\n",
      "Saving batch 8350/9870 to CSV...\n",
      "Saving batch 8400/9870 to CSV...\n",
      "Saving batch 8450/9870 to CSV...\n",
      "Saving batch 8500/9870 to CSV...\n",
      "Saving batch 8550/9870 to CSV...\n",
      "Saving batch 8600/9870 to CSV...\n",
      "Saving batch 8650/9870 to CSV...\n",
      "Saving batch 8700/9870 to CSV...\n",
      "Saving batch 8750/9870 to CSV...\n",
      "Saving batch 8800/9870 to CSV...\n",
      "Saving batch 8850/9870 to CSV...\n",
      "Saving batch 8900/9870 to CSV...\n",
      "Saving batch 8950/9870 to CSV...\n",
      "Saving batch 9000/9870 to CSV...\n",
      "Saving batch 9050/9870 to CSV...\n",
      "Saving batch 9100/9870 to CSV...\n",
      "Saving batch 9150/9870 to CSV...\n",
      "Saving batch 9200/9870 to CSV...\n",
      "Saving batch 9250/9870 to CSV...\n",
      "Saving batch 9300/9870 to CSV...\n",
      "Saving batch 9350/9870 to CSV...\n",
      "Saving batch 9400/9870 to CSV...\n",
      "Saving batch 9450/9870 to CSV...\n",
      "Saving batch 9500/9870 to CSV...\n",
      "Saving batch 9550/9870 to CSV...\n",
      "Saving batch 9600/9870 to CSV...\n",
      "Saving batch 9650/9870 to CSV...\n",
      "Saving batch 9700/9870 to CSV...\n",
      "Saving batch 9750/9870 to CSV...\n",
      "Saving batch 9800/9870 to CSV...\n",
      "Saving batch 9850/9870 to CSV...\n",
      "Processing complete!\n"
     ]
    }
   ],
   "source": [
    "#BEST TILL NOW\n",
    "import easyocr\n",
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "import cv2\n",
    "import gc\n",
    "import torch\n",
    "import os\n",
    "\n",
    "# Set PyTorch environment variable to handle memory fragmentation\n",
    "os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'\n",
    "\n",
    "# Initialize the EasyOCR Reader to use GPU (gpu=True)\n",
    "reader = easyocr.Reader(['en'], gpu=True)\n",
    "\n",
    "# Function to correct OCR misinterpretations based on context\n",
    "def correct_ocr_text(text):\n",
    "    words = text.split()\n",
    "    corrected_words = []\n",
    "    for word in words:\n",
    "        if re.search(r'\\d', word):\n",
    "            corrected_word = re.sub(r'[Oo]', '0', word)\n",
    "        else:\n",
    "            corrected_word = word\n",
    "        corrected_words.append(corrected_word)\n",
    "    return ' '.join(corrected_words)\n",
    "\n",
    "# Function to clear GPU memory\n",
    "def clear_gpu_memory():\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "\n",
    "# Function to resize large images to reduce memory consumption\n",
    "def resize_image(image, max_size=1024, min_size=300):\n",
    "    h, w = image.shape\n",
    "    if max(h, w) > max_size:\n",
    "        scale = max_size / float(max(h, w))\n",
    "        return cv2.resize(image, (int(w * scale), int(h * scale)))\n",
    "    elif min(h, w) < min_size:\n",
    "        # Skip resizing for small images to preserve detail\n",
    "        return image\n",
    "    return image\n",
    "\n",
    "# Function to process images in batches with optimized GPU handling\n",
    "def process_images_in_folder(folder_path, csv_output_file, batch_size=50):\n",
    "    # Read the existing CSV file to determine which images have already been processed\n",
    "    if os.path.exists(csv_output_file):\n",
    "        processed_df = pd.read_csv(csv_output_file)\n",
    "        processed_images = set(processed_df['image_file'].tolist())\n",
    "    else:\n",
    "        processed_images = set()\n",
    "\n",
    "    results = []\n",
    "    image_files = [f for f in os.listdir(folder_path) if f.endswith(('.png', '.jpg', '.jpeg'))]\n",
    "    total_images = len(image_files)\n",
    "    print(f\"Total images to process: {total_images}\")\n",
    "\n",
    "    for idx, image_file in enumerate(image_files):\n",
    "        if image_file in processed_images:\n",
    "            continue  # Skip already processed images\n",
    "\n",
    "        try:\n",
    "            # Load and resize the grayscale image\n",
    "            image_path = os.path.join(folder_path, image_file)\n",
    "            img = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)\n",
    "            \n",
    "            if img is None:\n",
    "                print(f\"Error reading image {image_file}\")\n",
    "                continue\n",
    "\n",
    "            # Resize image if too large to save memory\n",
    "            img = resize_image(img, max_size=1024)\n",
    "\n",
    "            # Use EasyOCR to extract text\n",
    "            result = reader.readtext(img)\n",
    "            extracted_text = ' '.join([item[1] for item in result])\n",
    "\n",
    "            # Correct OCR errors\n",
    "            corrected_text = correct_ocr_text(extracted_text)\n",
    "            results.append({'image_file': image_file, 'extracted_text': corrected_text})\n",
    "\n",
    "            # Save to CSV in batches\n",
    "            if (idx + 1) % batch_size == 0:\n",
    "                print(f\"Saving batch {idx + 1}/{total_images} to CSV...\")\n",
    "                results_df = pd.DataFrame(results, columns=['image_file', 'extracted_text'])\n",
    "                if os.path.exists(csv_output_file):\n",
    "                    results_df.to_csv(csv_output_file, mode='a', header=False, index=False)\n",
    "                else:\n",
    "                    results_df.to_csv(csv_output_file, index=False)\n",
    "                results = []  # Clear the batch after saving\n",
    "\n",
    "                # Clear GPU memory after every batch\n",
    "                clear_gpu_memory()\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {image_file}: {e}\")\n",
    "            results.append({'image_file': image_file, 'extracted_text': '', 'error': str(e)})\n",
    "            \n",
    "            # Clear GPU memory in case of errors\n",
    "            clear_gpu_memory()\n",
    "\n",
    "        # Clear GPU memory more frequently after each image\n",
    "        clear_gpu_memory()\n",
    "\n",
    "    # Save remaining results\n",
    "    if results:\n",
    "        results_df = pd.DataFrame(results, columns=['image_file', 'extracted_text'])\n",
    "        if os.path.exists(csv_output_file):\n",
    "            results_df.to_csv(csv_output_file, mode='a', header=False, index=False)\n",
    "        else:\n",
    "            results_df.to_csv(csv_output_file, index=False)\n",
    "\n",
    "    print(\"Processing complete!\")\n",
    "\n",
    "# Example usage: Process images from the folder and resume from where it stopped\n",
    "folder_path = r'images_7'\n",
    "csv_output_file = \"train_image_7.csv\"\n",
    "\n",
    "# Resume processing images and save results in batches of 50 images\n",
    "process_images_in_folder(folder_path, csv_output_file, batch_size=50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total images to process: 9895\n",
      "Saving batch 50/9895 to CSV...\n",
      "Saving batch 100/9895 to CSV...\n",
      "Saving batch 150/9895 to CSV...\n",
      "Saving batch 200/9895 to CSV...\n",
      "Saving batch 250/9895 to CSV...\n",
      "Saving batch 300/9895 to CSV...\n",
      "Saving batch 350/9895 to CSV...\n",
      "Saving batch 400/9895 to CSV...\n",
      "Saving batch 450/9895 to CSV...\n",
      "Saving batch 500/9895 to CSV...\n",
      "Saving batch 550/9895 to CSV...\n",
      "Saving batch 600/9895 to CSV...\n",
      "Saving batch 650/9895 to CSV...\n",
      "Saving batch 700/9895 to CSV...\n",
      "Saving batch 750/9895 to CSV...\n",
      "Saving batch 800/9895 to CSV...\n",
      "Saving batch 850/9895 to CSV...\n",
      "Saving batch 900/9895 to CSV...\n",
      "Saving batch 950/9895 to CSV...\n",
      "Saving batch 1000/9895 to CSV...\n",
      "Saving batch 1050/9895 to CSV...\n",
      "Saving batch 1100/9895 to CSV...\n",
      "Saving batch 1150/9895 to CSV...\n",
      "Saving batch 1200/9895 to CSV...\n",
      "Saving batch 1250/9895 to CSV...\n",
      "Saving batch 1300/9895 to CSV...\n",
      "Saving batch 1350/9895 to CSV...\n",
      "Saving batch 1400/9895 to CSV...\n",
      "Saving batch 1450/9895 to CSV...\n",
      "Saving batch 1500/9895 to CSV...\n",
      "Saving batch 1550/9895 to CSV...\n",
      "Saving batch 1600/9895 to CSV...\n",
      "Saving batch 1650/9895 to CSV...\n",
      "Saving batch 1700/9895 to CSV...\n",
      "Saving batch 1750/9895 to CSV...\n",
      "Saving batch 1800/9895 to CSV...\n",
      "Saving batch 1850/9895 to CSV...\n",
      "Saving batch 1900/9895 to CSV...\n",
      "Saving batch 1950/9895 to CSV...\n",
      "Saving batch 2000/9895 to CSV...\n",
      "Saving batch 2050/9895 to CSV...\n",
      "Saving batch 2100/9895 to CSV...\n",
      "Saving batch 2150/9895 to CSV...\n",
      "Saving batch 2200/9895 to CSV...\n",
      "Saving batch 2250/9895 to CSV...\n",
      "Saving batch 2300/9895 to CSV...\n",
      "Saving batch 2350/9895 to CSV...\n",
      "Saving batch 2400/9895 to CSV...\n",
      "Saving batch 2450/9895 to CSV...\n",
      "Saving batch 2500/9895 to CSV...\n",
      "Saving batch 2550/9895 to CSV...\n",
      "Saving batch 2600/9895 to CSV...\n",
      "Saving batch 2650/9895 to CSV...\n",
      "Saving batch 2700/9895 to CSV...\n",
      "Saving batch 2750/9895 to CSV...\n",
      "Saving batch 2800/9895 to CSV...\n",
      "Saving batch 2850/9895 to CSV...\n",
      "Saving batch 2900/9895 to CSV...\n",
      "Saving batch 2950/9895 to CSV...\n",
      "Saving batch 3000/9895 to CSV...\n",
      "Saving batch 3050/9895 to CSV...\n",
      "Saving batch 3100/9895 to CSV...\n",
      "Saving batch 3150/9895 to CSV...\n",
      "Saving batch 3200/9895 to CSV...\n",
      "Saving batch 3250/9895 to CSV...\n",
      "Saving batch 3300/9895 to CSV...\n",
      "Saving batch 3350/9895 to CSV...\n",
      "Saving batch 3400/9895 to CSV...\n",
      "Saving batch 3450/9895 to CSV...\n",
      "Saving batch 3500/9895 to CSV...\n",
      "Saving batch 3550/9895 to CSV...\n",
      "Saving batch 3600/9895 to CSV...\n",
      "Saving batch 3650/9895 to CSV...\n",
      "Saving batch 3700/9895 to CSV...\n",
      "Saving batch 3750/9895 to CSV...\n",
      "Saving batch 3800/9895 to CSV...\n",
      "Saving batch 3850/9895 to CSV...\n",
      "Saving batch 3900/9895 to CSV...\n",
      "Saving batch 3950/9895 to CSV...\n",
      "Saving batch 4000/9895 to CSV...\n",
      "Saving batch 4050/9895 to CSV...\n",
      "Saving batch 4100/9895 to CSV...\n",
      "Saving batch 4150/9895 to CSV...\n",
      "Saving batch 4200/9895 to CSV...\n",
      "Saving batch 4250/9895 to CSV...\n",
      "Saving batch 4300/9895 to CSV...\n",
      "Saving batch 4350/9895 to CSV...\n",
      "Saving batch 4400/9895 to CSV...\n",
      "Saving batch 4450/9895 to CSV...\n",
      "Saving batch 4500/9895 to CSV...\n",
      "Saving batch 4550/9895 to CSV...\n",
      "Saving batch 4600/9895 to CSV...\n",
      "Saving batch 4650/9895 to CSV...\n",
      "Saving batch 4700/9895 to CSV...\n",
      "Saving batch 4750/9895 to CSV...\n",
      "Saving batch 4800/9895 to CSV...\n",
      "Saving batch 4850/9895 to CSV...\n",
      "Saving batch 4900/9895 to CSV...\n",
      "Saving batch 4950/9895 to CSV...\n",
      "Saving batch 5000/9895 to CSV...\n",
      "Saving batch 5050/9895 to CSV...\n",
      "Saving batch 5100/9895 to CSV...\n",
      "Saving batch 5150/9895 to CSV...\n",
      "Saving batch 5200/9895 to CSV...\n",
      "Saving batch 5250/9895 to CSV...\n",
      "Saving batch 5300/9895 to CSV...\n",
      "Saving batch 5350/9895 to CSV...\n",
      "Saving batch 5400/9895 to CSV...\n",
      "Saving batch 5450/9895 to CSV...\n",
      "Saving batch 5500/9895 to CSV...\n",
      "Saving batch 5550/9895 to CSV...\n",
      "Saving batch 5600/9895 to CSV...\n",
      "Saving batch 5650/9895 to CSV...\n",
      "Saving batch 5700/9895 to CSV...\n",
      "Saving batch 5750/9895 to CSV...\n",
      "Saving batch 5800/9895 to CSV...\n",
      "Saving batch 5850/9895 to CSV...\n",
      "Saving batch 5900/9895 to CSV...\n",
      "Saving batch 5950/9895 to CSV...\n",
      "Saving batch 6000/9895 to CSV...\n",
      "Saving batch 6050/9895 to CSV...\n",
      "Saving batch 6100/9895 to CSV...\n",
      "Saving batch 6150/9895 to CSV...\n",
      "Saving batch 6200/9895 to CSV...\n",
      "Saving batch 6250/9895 to CSV...\n",
      "Saving batch 6300/9895 to CSV...\n",
      "Saving batch 6350/9895 to CSV...\n",
      "Saving batch 6400/9895 to CSV...\n",
      "Saving batch 6450/9895 to CSV...\n",
      "Saving batch 6500/9895 to CSV...\n",
      "Saving batch 6550/9895 to CSV...\n",
      "Saving batch 6600/9895 to CSV...\n",
      "Saving batch 6650/9895 to CSV...\n",
      "Saving batch 6700/9895 to CSV...\n",
      "Saving batch 6750/9895 to CSV...\n",
      "Saving batch 6800/9895 to CSV...\n",
      "Saving batch 6850/9895 to CSV...\n",
      "Saving batch 6900/9895 to CSV...\n",
      "Saving batch 6950/9895 to CSV...\n",
      "Saving batch 7000/9895 to CSV...\n",
      "Saving batch 7050/9895 to CSV...\n",
      "Saving batch 7100/9895 to CSV...\n",
      "Saving batch 7150/9895 to CSV...\n",
      "Saving batch 7200/9895 to CSV...\n",
      "Saving batch 7250/9895 to CSV...\n",
      "Saving batch 7300/9895 to CSV...\n",
      "Saving batch 7350/9895 to CSV...\n",
      "Saving batch 7400/9895 to CSV...\n",
      "Saving batch 7450/9895 to CSV...\n",
      "Saving batch 7500/9895 to CSV...\n",
      "Saving batch 7550/9895 to CSV...\n",
      "Saving batch 7600/9895 to CSV...\n",
      "Saving batch 7650/9895 to CSV...\n",
      "Saving batch 7700/9895 to CSV...\n",
      "Saving batch 7750/9895 to CSV...\n",
      "Saving batch 7800/9895 to CSV...\n",
      "Saving batch 7850/9895 to CSV...\n",
      "Saving batch 7900/9895 to CSV...\n",
      "Saving batch 7950/9895 to CSV...\n",
      "Saving batch 8000/9895 to CSV...\n",
      "Saving batch 8050/9895 to CSV...\n",
      "Saving batch 8100/9895 to CSV...\n",
      "Saving batch 8150/9895 to CSV...\n",
      "Saving batch 8200/9895 to CSV...\n",
      "Saving batch 8250/9895 to CSV...\n",
      "Saving batch 8300/9895 to CSV...\n",
      "Saving batch 8350/9895 to CSV...\n",
      "Saving batch 8400/9895 to CSV...\n"
     ]
    }
   ],
   "source": [
    "#BEST TILL NOW\n",
    "import easyocr\n",
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "import cv2\n",
    "import gc\n",
    "import torch\n",
    "import os\n",
    "\n",
    "# Set PyTorch environment variable to handle memory fragmentation\n",
    "os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'\n",
    "\n",
    "# Initialize the EasyOCR Reader to use GPU (gpu=True)\n",
    "reader = easyocr.Reader(['en'], gpu=True)\n",
    "\n",
    "# Function to correct OCR misinterpretations based on context\n",
    "def correct_ocr_text(text):\n",
    "    words = text.split()\n",
    "    corrected_words = []\n",
    "    for word in words:\n",
    "        if re.search(r'\\d', word):\n",
    "            corrected_word = re.sub(r'[Oo]', '0', word)\n",
    "        else:\n",
    "            corrected_word = word\n",
    "        corrected_words.append(corrected_word)\n",
    "    return ' '.join(corrected_words)\n",
    "\n",
    "# Function to clear GPU memory\n",
    "def clear_gpu_memory():\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "\n",
    "# Function to resize large images to reduce memory consumption\n",
    "def resize_image(image, max_size=1024, min_size=300):\n",
    "    h, w = image.shape\n",
    "    if max(h, w) > max_size:\n",
    "        scale = max_size / float(max(h, w))\n",
    "        return cv2.resize(image, (int(w * scale), int(h * scale)))\n",
    "    elif min(h, w) < min_size:\n",
    "        # Skip resizing for small images to preserve detail\n",
    "        return image\n",
    "    return image\n",
    "\n",
    "# Function to process images in batches with optimized GPU handling\n",
    "def process_images_in_folder(folder_path, csv_output_file, batch_size=50):\n",
    "    # Read the existing CSV file to determine which images have already been processed\n",
    "    if os.path.exists(csv_output_file):\n",
    "        processed_df = pd.read_csv(csv_output_file)\n",
    "        processed_images = set(processed_df['image_file'].tolist())\n",
    "    else:\n",
    "        processed_images = set()\n",
    "\n",
    "    results = []\n",
    "    image_files = [f for f in os.listdir(folder_path) if f.endswith(('.png', '.jpg', '.jpeg'))]\n",
    "    total_images = len(image_files)\n",
    "    print(f\"Total images to process: {total_images}\")\n",
    "\n",
    "    for idx, image_file in enumerate(image_files):\n",
    "        if image_file in processed_images:\n",
    "            continue  # Skip already processed images\n",
    "\n",
    "        try:\n",
    "            # Load and resize the grayscale image\n",
    "            image_path = os.path.join(folder_path, image_file)\n",
    "            img = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)\n",
    "            \n",
    "            if img is None:\n",
    "                print(f\"Error reading image {image_file}\")\n",
    "                continue\n",
    "\n",
    "            # Resize image if too large to save memory\n",
    "            img = resize_image(img, max_size=1024)\n",
    "\n",
    "            # Use EasyOCR to extract text\n",
    "            result = reader.readtext(img)\n",
    "            extracted_text = ' '.join([item[1] for item in result])\n",
    "\n",
    "            # Correct OCR errors\n",
    "            corrected_text = correct_ocr_text(extracted_text)\n",
    "            results.append({'image_file': image_file, 'extracted_text': corrected_text})\n",
    "\n",
    "            # Save to CSV in batches\n",
    "            if (idx + 1) % batch_size == 0:\n",
    "                print(f\"Saving batch {idx + 1}/{total_images} to CSV...\")\n",
    "                results_df = pd.DataFrame(results, columns=['image_file', 'extracted_text'])\n",
    "                if os.path.exists(csv_output_file):\n",
    "                    results_df.to_csv(csv_output_file, mode='a', header=False, index=False)\n",
    "                else:\n",
    "                    results_df.to_csv(csv_output_file, index=False)\n",
    "                results = []  # Clear the batch after saving\n",
    "\n",
    "                # Clear GPU memory after every batch\n",
    "                clear_gpu_memory()\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {image_file}: {e}\")\n",
    "            results.append({'image_file': image_file, 'extracted_text': '', 'error': str(e)})\n",
    "            \n",
    "            # Clear GPU memory in case of errors\n",
    "            clear_gpu_memory()\n",
    "\n",
    "        # Clear GPU memory more frequently after each image\n",
    "        clear_gpu_memory()\n",
    "\n",
    "    # Save remaining results\n",
    "    if results:\n",
    "        results_df = pd.DataFrame(results, columns=['image_file', 'extracted_text'])\n",
    "        if os.path.exists(csv_output_file):\n",
    "            results_df.to_csv(csv_output_file, mode='a', header=False, index=False)\n",
    "        else:\n",
    "            results_df.to_csv(csv_output_file, index=False)\n",
    "\n",
    "    print(\"Processing complete!\")\n",
    "\n",
    "# Example usage: Process images from the folder and resume from where it stopped\n",
    "folder_path = r'images_10'\n",
    "csv_output_file = \"train_image_10.csv\"\n",
    "\n",
    "# Resume processing images and save results in batches of 50 images\n",
    "process_images_in_folder(folder_path, csv_output_file, batch_size=50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#BEST TILL NOW\n",
    "import easyocr\n",
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "import cv2\n",
    "import gc\n",
    "import torch\n",
    "import os\n",
    "\n",
    "# Set PyTorch environment variable to handle memory fragmentation\n",
    "os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'\n",
    "\n",
    "# Initialize the EasyOCR Reader to use GPU (gpu=True)\n",
    "reader = easyocr.Reader(['en'], gpu=True)\n",
    "\n",
    "# Function to correct OCR misinterpretations based on context\n",
    "def correct_ocr_text(text):\n",
    "    words = text.split()\n",
    "    corrected_words = []\n",
    "    for word in words:\n",
    "        if re.search(r'\\d', word):\n",
    "            corrected_word = re.sub(r'[Oo]', '0', word)\n",
    "        else:\n",
    "            corrected_word = word\n",
    "        corrected_words.append(corrected_word)\n",
    "    return ' '.join(corrected_words)\n",
    "\n",
    "# Function to clear GPU memory\n",
    "def clear_gpu_memory():\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "\n",
    "# Function to resize large images to reduce memory consumption\n",
    "def resize_image(image, max_size=1024, min_size=300):\n",
    "    h, w = image.shape\n",
    "    if max(h, w) > max_size:\n",
    "        scale = max_size / float(max(h, w))\n",
    "        return cv2.resize(image, (int(w * scale), int(h * scale)))\n",
    "    elif min(h, w) < min_size:\n",
    "        # Skip resizing for small images to preserve detail\n",
    "        return image\n",
    "    return image\n",
    "\n",
    "# Function to process images in batches with optimized GPU handling\n",
    "def process_images_in_folder(folder_path, csv_output_file, batch_size=50):\n",
    "    # Read the existing CSV file to determine which images have already been processed\n",
    "    if os.path.exists(csv_output_file):\n",
    "        processed_df = pd.read_csv(csv_output_file)\n",
    "        processed_images = set(processed_df['image_file'].tolist())\n",
    "    else:\n",
    "        processed_images = set()\n",
    "\n",
    "    results = []\n",
    "    image_files = [f for f in os.listdir(folder_path) if f.endswith(('.png', '.jpg', '.jpeg'))]\n",
    "    total_images = len(image_files)\n",
    "    print(f\"Total images to process: {total_images}\")\n",
    "\n",
    "    for idx, image_file in enumerate(image_files):\n",
    "        if image_file in processed_images:\n",
    "            continue  # Skip already processed images\n",
    "\n",
    "        try:\n",
    "            # Load and resize the grayscale image\n",
    "            image_path = os.path.join(folder_path, image_file)\n",
    "            img = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)\n",
    "            \n",
    "            if img is None:\n",
    "                print(f\"Error reading image {image_file}\")\n",
    "                continue\n",
    "\n",
    "            # Resize image if too large to save memory\n",
    "            img = resize_image(img, max_size=1024)\n",
    "\n",
    "            # Use EasyOCR to extract text\n",
    "            result = reader.readtext(img)\n",
    "            extracted_text = ' '.join([item[1] for item in result])\n",
    "\n",
    "            # Correct OCR errors\n",
    "            corrected_text = correct_ocr_text(extracted_text)\n",
    "            results.append({'image_file': image_file, 'extracted_text': corrected_text})\n",
    "\n",
    "            # Save to CSV in batches\n",
    "            if (idx + 1) % batch_size == 0:\n",
    "                print(f\"Saving batch {idx + 1}/{total_images} to CSV...\")\n",
    "                results_df = pd.DataFrame(results, columns=['image_file', 'extracted_text'])\n",
    "                if os.path.exists(csv_output_file):\n",
    "                    results_df.to_csv(csv_output_file, mode='a', header=False, index=False)\n",
    "                else:\n",
    "                    results_df.to_csv(csv_output_file, index=False)\n",
    "                results = []  # Clear the batch after saving\n",
    "\n",
    "                # Clear GPU memory after every batch\n",
    "                clear_gpu_memory()\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {image_file}: {e}\")\n",
    "            results.append({'image_file': image_file, 'extracted_text': '', 'error': str(e)})\n",
    "            \n",
    "            # Clear GPU memory in case of errors\n",
    "            clear_gpu_memory()\n",
    "\n",
    "        # Clear GPU memory more frequently after each image\n",
    "        clear_gpu_memory()\n",
    "\n",
    "    # Save remaining results\n",
    "    if results:\n",
    "        results_df = pd.DataFrame(results, columns=['image_file', 'extracted_text'])\n",
    "        if os.path.exists(csv_output_file):\n",
    "            results_df.to_csv(csv_output_file, mode='a', header=False, index=False)\n",
    "        else:\n",
    "            results_df.to_csv(csv_output_file, index=False)\n",
    "\n",
    "    print(\"Processing complete!\")\n",
    "\n",
    "# Example usage: Process images from the folder and resume from where it stopped\n",
    "folder_path = r'images_11'\n",
    "csv_output_file = \"train_image_11.csv\"\n",
    "\n",
    "# Resume processing images and save results in batches of 50 images\n",
    "process_images_in_folder(folder_path, csv_output_file, batch_size=50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "val = train[70000:80000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "xval = val[\"image_link\"].values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10000/10000 [01:29<00:00, 112.18it/s]\n"
     ]
    }
   ],
   "source": [
    "from utils import download_images\n",
    "download_images(xval, \"images_11\"  , allow_multiprocessing=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "expected string or bytes-like object, got 'float'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 55\u001b[0m\n\u001b[0;32m     52\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m matches\n\u001b[0;32m     54\u001b[0m \u001b[38;5;66;03m# Apply the extraction function to the 'extracted_text' column\u001b[39;00m\n\u001b[1;32m---> 55\u001b[0m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mextracted_values\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mextracted_text\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mextract_values\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     57\u001b[0m \u001b[38;5;66;03m# Display the DataFrame with the extracted values#\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\core\\series.py:4924\u001b[0m, in \u001b[0;36mSeries.apply\u001b[1;34m(self, func, convert_dtype, args, by_row, **kwargs)\u001b[0m\n\u001b[0;32m   4789\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply\u001b[39m(\n\u001b[0;32m   4790\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   4791\u001b[0m     func: AggFuncType,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   4796\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m   4797\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame \u001b[38;5;241m|\u001b[39m Series:\n\u001b[0;32m   4798\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   4799\u001b[0m \u001b[38;5;124;03m    Invoke function on values of Series.\u001b[39;00m\n\u001b[0;32m   4800\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   4915\u001b[0m \u001b[38;5;124;03m    dtype: float64\u001b[39;00m\n\u001b[0;32m   4916\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m   4917\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mSeriesApply\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   4918\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4919\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4920\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconvert_dtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconvert_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4921\u001b[0m \u001b[43m        \u001b[49m\u001b[43mby_row\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mby_row\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4922\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4923\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m-> 4924\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\core\\apply.py:1427\u001b[0m, in \u001b[0;36mSeriesApply.apply\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1424\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_compat()\n\u001b[0;32m   1426\u001b[0m \u001b[38;5;66;03m# self.func is Callable\u001b[39;00m\n\u001b[1;32m-> 1427\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_standard\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\core\\apply.py:1507\u001b[0m, in \u001b[0;36mSeriesApply.apply_standard\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1501\u001b[0m \u001b[38;5;66;03m# row-wise access\u001b[39;00m\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# apply doesn't have a `na_action` keyword and for backward compat reasons\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m \u001b[38;5;66;03m# we need to give `na_action=\"ignore\"` for categorical data.\u001b[39;00m\n\u001b[0;32m   1504\u001b[0m \u001b[38;5;66;03m# TODO: remove the `na_action=\"ignore\"` when that default has been changed in\u001b[39;00m\n\u001b[0;32m   1505\u001b[0m \u001b[38;5;66;03m#  Categorical (GH51645).\u001b[39;00m\n\u001b[0;32m   1506\u001b[0m action \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(obj\u001b[38;5;241m.\u001b[39mdtype, CategoricalDtype) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1507\u001b[0m mapped \u001b[38;5;241m=\u001b[39m \u001b[43mobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_map_values\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1508\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmapper\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcurried\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mna_action\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maction\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert_dtype\u001b[49m\n\u001b[0;32m   1509\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1511\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(mapped) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(mapped[\u001b[38;5;241m0\u001b[39m], ABCSeries):\n\u001b[0;32m   1512\u001b[0m     \u001b[38;5;66;03m# GH#43986 Need to do list(mapped) in order to get treated as nested\u001b[39;00m\n\u001b[0;32m   1513\u001b[0m     \u001b[38;5;66;03m#  See also GH#25959 regarding EA support\u001b[39;00m\n\u001b[0;32m   1514\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m obj\u001b[38;5;241m.\u001b[39m_constructor_expanddim(\u001b[38;5;28mlist\u001b[39m(mapped), index\u001b[38;5;241m=\u001b[39mobj\u001b[38;5;241m.\u001b[39mindex)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\core\\base.py:921\u001b[0m, in \u001b[0;36mIndexOpsMixin._map_values\u001b[1;34m(self, mapper, na_action, convert)\u001b[0m\n\u001b[0;32m    918\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(arr, ExtensionArray):\n\u001b[0;32m    919\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m arr\u001b[38;5;241m.\u001b[39mmap(mapper, na_action\u001b[38;5;241m=\u001b[39mna_action)\n\u001b[1;32m--> 921\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43malgorithms\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43marr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmapper\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mna_action\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mna_action\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\core\\algorithms.py:1743\u001b[0m, in \u001b[0;36mmap_array\u001b[1;34m(arr, mapper, na_action, convert)\u001b[0m\n\u001b[0;32m   1741\u001b[0m values \u001b[38;5;241m=\u001b[39m arr\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mobject\u001b[39m, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m na_action \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1743\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap_infer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmapper\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1745\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m lib\u001b[38;5;241m.\u001b[39mmap_infer_mask(\n\u001b[0;32m   1746\u001b[0m         values, mapper, mask\u001b[38;5;241m=\u001b[39misna(values)\u001b[38;5;241m.\u001b[39mview(np\u001b[38;5;241m.\u001b[39muint8), convert\u001b[38;5;241m=\u001b[39mconvert\n\u001b[0;32m   1747\u001b[0m     )\n",
      "File \u001b[1;32mlib.pyx:2972\u001b[0m, in \u001b[0;36mpandas._libs.lib.map_infer\u001b[1;34m()\u001b[0m\n",
      "Cell \u001b[1;32mIn[4], line 49\u001b[0m, in \u001b[0;36mextract_values\u001b[1;34m(text)\u001b[0m\n\u001b[0;32m     47\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mextract_values\u001b[39m(text):\n\u001b[0;32m     48\u001b[0m     \u001b[38;5;66;03m# Match numbers with optional commas and decimals, followed by a unit\u001b[39;00m\n\u001b[1;32m---> 49\u001b[0m     matches \u001b[38;5;241m=\u001b[39m \u001b[43mre\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfindall\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mrf\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m(\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43md\u001b[39;49m\u001b[38;5;130;43;01m{{\u001b[39;49;00m\u001b[38;5;124;43m1,3\u001b[39;49m\u001b[38;5;130;43;01m}}\u001b[39;49;00m\u001b[38;5;124;43m(?:,\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43md\u001b[39;49m\u001b[38;5;130;43;01m{{\u001b[39;49;00m\u001b[38;5;124;43m3\u001b[39;49m\u001b[38;5;130;43;01m}}\u001b[39;49;00m\u001b[38;5;124;43m)*(?:\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43m.\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43md+)?)\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43ms*(\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43munit_pattern\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m)\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mre\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mIGNORECASE\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     50\u001b[0m     \u001b[38;5;66;03m# Remove commas from the numeric part\u001b[39;00m\n\u001b[0;32m     51\u001b[0m     matches \u001b[38;5;241m=\u001b[39m [(re\u001b[38;5;241m.\u001b[39msub(\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m, num), unit) \u001b[38;5;28;01mfor\u001b[39;00m num, unit \u001b[38;5;129;01min\u001b[39;00m matches]\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\re\\__init__.py:217\u001b[0m, in \u001b[0;36mfindall\u001b[1;34m(pattern, string, flags)\u001b[0m\n\u001b[0;32m    209\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfindall\u001b[39m(pattern, string, flags\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m):\n\u001b[0;32m    210\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Return a list of all non-overlapping matches in the string.\u001b[39;00m\n\u001b[0;32m    211\u001b[0m \n\u001b[0;32m    212\u001b[0m \u001b[38;5;124;03m    If one or more capturing groups are present in the pattern, return\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    215\u001b[0m \n\u001b[0;32m    216\u001b[0m \u001b[38;5;124;03m    Empty matches are included in the result.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 217\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_compile\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpattern\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mflags\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfindall\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstring\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mTypeError\u001b[0m: expected string or bytes-like object, got 'float'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# Load the CSV file\n",
    "file_path = \"train_image_4.csv\"  # Replace with the actual path to your CSV file\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Define a mapping for unit abbreviations\n",
    "unit_variants = {\n",
    "    \"centimetre\": [\"cm\", \"centimetre\"],\n",
    "    \"foot\": [\"ft\", \"foot\"],\n",
    "    \"millimetre\": [\"mm\", \"millimetre\"],\n",
    "    \"metre\": [\"m\", \"metre\"],\n",
    "    \"inch\": [\"in\", \"inch\"],\n",
    "    \"yard\": [\"yd\", \"yard\"],\n",
    "    \"milligram\": [\"mg\", \"milligram\"],\n",
    "    \"kilogram\": [\"kg\", \"kilogram\"],\n",
    "    \"microgram\": [\"Âµg\", \"microgram\"],\n",
    "    \"gram\": [\"g\", \"gram\"],\n",
    "    \"ounce\": [\"oz\", \"ounce\"],\n",
    "    \"ton\": [\"ton\"],\n",
    "    \"pound\": [\"lb\", \"pound\"],\n",
    "    \"millivolt\": [\"mv\", \"millivolt\"],\n",
    "    \"kilovolt\": [\"kv\", \"kilovolt\"],\n",
    "    \"volt\": [\"v\", \"volt\"],\n",
    "    \"kilowatt\": [\"kw\", \"kilowatt\"],\n",
    "    \"watt\": [\"w\", \"watt\"],\n",
    "    \"cubic foot\": [\"cubic foot\"],\n",
    "    \"microlitre\": [\"Âµl\", \"microlitre\"],\n",
    "    \"cup\": [\"cup\"],\n",
    "    \"fluid ounce\": [\"fl oz\", \"fluid ounce\"],\n",
    "    \"centilitre\": [\"cl\", \"centilitre\"],\n",
    "    \"imperial gallon\": [\"imperial gallon\"],\n",
    "    \"pint\": [\"pt\", \"pint\"],\n",
    "    \"decilitre\": [\"dl\", \"decilitre\"],\n",
    "    \"litre\": [\"l\", \"litre\"],\n",
    "    \"millilitre\": [\"ml\", \"millilitre\"],\n",
    "    \"quart\": [\"qt\", \"quart\"],\n",
    "    \"cubic inch\": [\"cubic inch\"],\n",
    "    \"gallon\": [\"gal\", \"gallon\"]\n",
    "}\n",
    "\n",
    "# Create a regex pattern for units with case insensitivity and variations\n",
    "unit_pattern = \"|\".join([f\"{abbrev}\" for unit, abbrevs in unit_variants.items() for abbrev in abbrevs])\n",
    "\n",
    "# Define regex to handle commas in numbers and case insensitivity for units\n",
    "def extract_values(text):\n",
    "    # Match numbers with optional commas and decimals, followed by a unit\n",
    "    matches = re.findall(rf'(\\d{{1,3}}(?:,\\d{{3}})*(?:\\.\\d+)?)\\s*({unit_pattern})', text, re.IGNORECASE)\n",
    "    # Remove commas from the numeric part\n",
    "    matches = [(re.sub(r',', '', num), unit) for num, unit in matches]\n",
    "    return matches\n",
    "\n",
    "# Apply the extraction function to the 'extracted_text' column\n",
    "df['extracted_values'] = df['extracted_text'].apply(extract_values)\n",
    "\n",
    "# Display the DataFrame with the extracted values#\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xvp = val[\"image_link\"].str.replace(\"https://m.media-amazon.com/images/I/\" , \"\")\n",
    "val[\"image_link\"] = xvp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_list = []\n",
    "image_path_list = df[\"image_file\"].values.tolist()\n",
    "update_images = []\n",
    "for v in image_path_list:\n",
    "    update_images.append(v)\n",
    "    #x = val[val[\"image_link\"] == v]\n",
    "    #if x.shape[0] == 1:\n",
    "        #update_images.append(val[val[\"image_link\"] == v][\"group_id\"].values.tolist()[0])\n",
    "print(update_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df = val[val['image_link'].isin(update_images)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_corpus = df[\"extracted_text\"].values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_numbers(input_string):\n",
    "    numbers = ''.join([char for char in input_string if char.isdigit()])\n",
    "    return numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alias_map = {\n",
    "    'centimetre': ['cm', 'centimetre', 'centimeter', 'centimetres', 'centimeters'],\n",
    "    'foot': ['ft', 'foot', 'feet'],\n",
    "    'inch': ['in', 'inch', 'inches'],\n",
    "    'metre': ['m', 'metre', 'meter', 'metres', 'meters'],\n",
    "    'millimetre': ['mm', 'millimetre', 'millimeter', 'millimetres', 'millimeters'],\n",
    "    'yard': ['yd', 'yard', 'yards'],\n",
    "\n",
    "    'gram': ['g', 'gm', 'gram', 'grams'],\n",
    "    'kilogram': ['kg', 'kilogram', 'kilograms'],\n",
    "    'microgram': ['mcg', 'Âµg', 'microgram', 'micrograms'],\n",
    "    'milligram': ['mg', 'milligram', 'milligrams'],\n",
    "    'ounce': ['oz', 'ounce', 'ounces'],\n",
    "    'pound': ['lb', 'lbs', 'pound', 'pounds'],\n",
    "    'ton': ['ton', 'tons', 'tonne', 'tonnes'],\n",
    "\n",
    "    'kilovolt': ['kV', 'kilovolt', 'kilovolts' , \"kv\"],\n",
    "    'millivolt': ['mV', 'millivolt', 'millivolts' , \"mv\"],\n",
    "    'volt': ['V', 'volt', 'volts'],\n",
    "\n",
    "    'kilowatt': ['kW', 'kilowatt', 'kilowatts'],\n",
    "    'watt': ['W', 'watt', 'watts'],\n",
    "\n",
    "    'centilitre': ['cl', 'centilitre', 'centilitres'],\n",
    "    'cubic foot': ['cu ft', 'cubic foot', 'cubic feet'],\n",
    "    'cubic inch': ['cu in', 'cubic inch', 'cubic inches'],\n",
    "    'cup': ['cup', 'cups'],\n",
    "    'decilitre': ['dl', 'decilitre', 'decilitres'],\n",
    "    'fluid ounce': ['fl oz', 'fluid ounce', 'fluid ounces'],\n",
    "    'gallon': ['gal', 'gallon', 'gallons'],\n",
    "    'imperial gallon': ['imp gal', 'imperial gallon', 'imperial gallons'],\n",
    "    'litre': ['l', 'litre', 'liter', 'litres', 'liters'],\n",
    "    'microlitre': ['Âµl', 'microlitre', 'microliter', 'microlitres', 'microliters'],\n",
    "    'millilitre': ['ml', 'millilitre', 'milliliter', 'millilitres', 'milliliters'],\n",
    "    'pint': ['pt', 'pint', 'pints'],\n",
    "    'quart': ['qt', 'quart', 'quarts']\n",
    "}\n",
    "\n",
    "lsit = []\n",
    "for key in alias_map:\n",
    "    lsit.extend(alias_map[key])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_unit(strg):\n",
    "    if strg in lsit:\n",
    "        return True\n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_counter = 0\n",
    "false_counter = 0\n",
    "flg = 1\n",
    "for idx , img in enumerate(filtered_df[\"image_link\"].values.tolist()):\n",
    "    text  = df[df[\"image_file\"] == img][\"extracted_values\"].values.tolist()[0]\n",
    "    acc_value = []\n",
    "    acc_unit = []\n",
    "    for t in text:\n",
    "        acc_unit.append(t[1])\n",
    "        acc_value.append(t[0])\n",
    "    value_x , unit_x = filtered_df[filtered_df[\"image_link\"] == img][\"entity_value\"].values[0].split(\" \")\n",
    "    value_x = float(value_x)\n",
    "    for idx , a_v in enumerate(acc_value):\n",
    "        x = float(a_v)\n",
    "        y = acc_unit[idx]\n",
    "        if x == value_x and check_unit(y):\n",
    "            true_counter += 1\n",
    "            flg = 0\n",
    "            break\n",
    "        else:\n",
    "            flg = 1\n",
    "    false_counter += flg\n",
    "            \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(true_counter)\n",
    "print(false_counter)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
