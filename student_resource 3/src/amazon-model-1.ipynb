{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":9386644,"sourceType":"datasetVersion","datasetId":5695335},{"sourceId":9386937,"sourceType":"datasetVersion","datasetId":5695561},{"sourceId":9387462,"sourceType":"datasetVersion","datasetId":5695991}],"dockerImageVersionId":30762,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"entity_unit_map = {\n    'width': {'centimetre', 'foot', 'inch', 'metre', 'millimetre', 'yard'},\n    'depth': {'centimetre', 'foot', 'inch', 'metre', 'millimetre', 'yard'},\n    'height': {'centimetre', 'foot', 'inch', 'metre', 'millimetre', 'yard'},\n    'item_weight': {'gram',\n        'kilogram',\n        'microgram',\n        'milligram',\n        'ounce',\n        'pound',\n        'ton'},\n    'maximum_weight_recommendation': {'gram',\n        'kilogram',\n        'microgram',\n        'milligram',\n        'ounce',\n        'pound',\n        'ton'},\n    'voltage': {'kilovolt', 'millivolt', 'volt'},\n    'wattage': {'kilowatt', 'watt'},\n    'item_volume': {'centilitre',\n        'cubic foot',\n        'cubic inch',\n        'cup',\n        'decilitre',\n        'fluid ounce',\n        'gallon',\n        'imperial gallon',\n        'litre',\n        'microlitre',\n        'millilitre',\n        'pint',\n        'quart'}\n}\n\nallowed_units = {unit for entity in entity_unit_map for unit in entity_unit_map[entity]}","metadata":{"execution":{"iopub.status.busy":"2024-09-13T22:02:59.318380Z","iopub.execute_input":"2024-09-13T22:02:59.318694Z","iopub.status.idle":"2024-09-13T22:02:59.331698Z","shell.execute_reply.started":"2024-09-13T22:02:59.318663Z","shell.execute_reply":"2024-09-13T22:02:59.330819Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"import re\nimport os\nimport requests\nimport pandas as pd\nimport multiprocessing\nimport time\nfrom time import time as timer\nfrom tqdm import tqdm\nimport numpy as np\nfrom pathlib import Path\nfrom functools import partial\nimport requests\nimport urllib\nfrom PIL import Image\n\ndef common_mistake(unit):\n    if unit in allowed_units:\n        return unit\n    if unit.replace('ter', 'tre') in allowed_units:\n        return unit.replace('ter', 'tre')\n    if unit.replace('feet', 'foot') in allowed_units:\n        return unit.replace('feet', 'foot')\n    return unit\n\ndef parse_string(s):\n    s_stripped = \"\" if s==None or str(s)=='nan' else s.strip()\n    if s_stripped == \"\":\n        return None, None\n    pattern = re.compile(r'^-?\\d+(\\.\\d+)?\\s+[a-zA-Z\\s]+$')\n    if not pattern.match(s_stripped):\n        raise ValueError(\"Invalid format in {}\".format(s))\n    parts = s_stripped.split(maxsplit=1)\n    number = float(parts[0])\n    unit = common_mistake(parts[1])\n    if unit not in allowed_units:\n        raise ValueError(\"Invalid unit [{}] found in {}. Allowed units: {}\".format(\n            unit, s, allowed_units))\n    return number, unit\n\n\ndef create_placeholder_image(image_save_path):\n    try:\n        placeholder_image = Image.new('RGB', (100, 100), color='black')\n        placeholder_image.save(image_save_path)\n    except Exception as e:\n        return\n\ndef download_image(image_link, save_folder, retries=3, delay=3):\n    if not isinstance(image_link, str):\n        return\n\n    filename = Path(image_link).name\n    image_save_path = os.path.join(save_folder, filename)\n\n    if os.path.exists(image_save_path):\n        return\n\n    for _ in range(retries):\n        try:\n            urllib.request.urlretrieve(image_link, image_save_path)\n            return\n        except:\n            time.sleep(delay)\n    \n    create_placeholder_image(image_save_path) \n\ndef download_images(image_links, download_folder, allow_multiprocessing=True):\n    if not os.path.exists(download_folder):\n        os.makedirs(download_folder)\n\n    if allow_multiprocessing:\n        download_image_partial = partial(\n            download_image, save_folder=download_folder, retries=3, delay=3)\n\n        with multiprocessing.Pool(64) as pool:\n            list(tqdm(pool.imap(download_image_partial, image_links), total=len(image_links)))\n            pool.close()\n            pool.join()\n    else:\n        for image_link in tqdm(image_links, total=len(image_links)):\n            download_image(image_link, save_folder=download_folder, retries=3, delay=3)\n        ","metadata":{"execution":{"iopub.status.busy":"2024-09-13T18:13:32.702547Z","iopub.execute_input":"2024-09-13T18:13:32.702932Z","iopub.status.idle":"2024-09-13T18:13:32.719944Z","shell.execute_reply.started":"2024-09-13T18:13:32.702894Z","shell.execute_reply":"2024-09-13T18:13:32.719153Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"import os \nimport json \nimport pandas as pd\nDATASET_FOLDER = '../dataset/'\ntrain = pd.read_csv(os.path.join(DATASET_FOLDER, '/kaggle/input/train-dataset/test.csv'))\ntest = pd.read_csv(os.path.join(DATASET_FOLDER, '/kaggle/input/train-dataset/train.csv'))","metadata":{"execution":{"iopub.status.busy":"2024-09-13T18:14:55.941036Z","iopub.execute_input":"2024-09-13T18:14:55.941929Z","iopub.status.idle":"2024-09-13T18:14:56.494502Z","shell.execute_reply.started":"2024-09-13T18:14:55.941885Z","shell.execute_reply":"2024-09-13T18:14:56.493688Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"download_image(link[0] , \"/kaggle/working/\")","metadata":{"execution":{"iopub.status.busy":"2024-09-13T18:17:26.490757Z","iopub.execute_input":"2024-09-13T18:17:26.491555Z","iopub.status.idle":"2024-09-13T18:17:26.495643Z","shell.execute_reply.started":"2024-09-13T18:17:26.491517Z","shell.execute_reply":"2024-09-13T18:17:26.494705Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import easyocr\nfrom PIL import Image\nimport pandas as pd\nimport os\nreader = easyocr.Reader(['en'])  \nimage_folder = 'Grayscaled/' \ndata = []\nimage_files = [f for f in os.listdir(image_folder) if f.endswith(('.png', '.jpg', '.jpeg'))]\nfor image_file in image_files:\n    image_path = os.path.join(image_folder, image_file)\n    try:\n        result = reader.readtext(image_path)\n        extracted_text = ' '.join([text[1] for text in result])\n        cleaned_text = extracted_text.strip().replace('\\n', ' ').replace('\\r', '')\n        data.append({'Image_File': image_file, 'Extracted_Text': cleaned_text})\n        \n    except Exception as e:\n        print(f\"An error occurred while processing {image_file}: {e}\")\ndf = pd.DataFrame(data)\ncsv_path = 'extracted_texts_easyocr.csv'\ndf.to_csv(csv_path, index=False)\nprint(f\"Data saved to {csv_path}\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader, Dataset\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder","metadata":{"execution":{"iopub.status.busy":"2024-09-13T19:41:35.436317Z","iopub.execute_input":"2024-09-13T19:41:35.437167Z","iopub.status.idle":"2024-09-13T19:41:35.442376Z","shell.execute_reply.started":"2024-09-13T19:41:35.437123Z","shell.execute_reply":"2024-09-13T19:41:35.441485Z"},"trusted":true},"execution_count":131,"outputs":[]},{"cell_type":"code","source":"class Seq2SeqDataset(Dataset):\n    def __init__(self, input_texts, target_texts, input_token_index, target_token_index):\n        self.input_texts = input_texts\n        self.target_texts = target_texts\n        self.input_token_index = input_token_index\n        self.target_token_index = target_token_index\n\n    def __len__(self):\n        return len(self.input_texts)\n\n    def __getitem__(self, idx):\n        input_text = self.input_texts[idx]\n        target_text = self.target_texts[idx]\n        input_seq = [self.input_token_index[char] for char in input_text]\n        target_seq = [self.target_token_index[char] for char in target_text]\n        return torch.tensor(input_seq, dtype=torch.long), torch.tensor(target_seq, dtype=torch.long)","metadata":{"execution":{"iopub.status.busy":"2024-09-13T19:41:52.232315Z","iopub.execute_input":"2024-09-13T19:41:52.232713Z","iopub.status.idle":"2024-09-13T19:41:52.239967Z","shell.execute_reply.started":"2024-09-13T19:41:52.232675Z","shell.execute_reply":"2024-09-13T19:41:52.238909Z"},"trusted":true},"execution_count":132,"outputs":[]},{"cell_type":"code","source":"class Encoder(nn.Module):\n    def __init__(self, input_dim, emb_dim, hid_dim, n_layers, dropout):\n        super().__init__()\n        self.embedding = nn.Embedding(input_dim, emb_dim)\n        self.rnn = nn.LSTM(emb_dim, hid_dim, n_layers, dropout=dropout)\n        self.dropout = nn.Dropout(dropout)\n\n    def forward(self, src):\n        embedded = self.dropout(self.embedding(src))\n        outputs, (hidden, cell) = self.rnn(embedded)\n        return hidden, cell\n\nclass Decoder(nn.Module):\n    def __init__(self, output_dim, emb_dim, hid_dim, n_layers, dropout):\n        super().__init__()\n        self.output_dim = output_dim\n        self.embedding = nn.Embedding(output_dim, emb_dim)\n        self.rnn = nn.LSTM(emb_dim, hid_dim, n_layers, dropout=dropout)\n        self.fc_out = nn.Linear(hid_dim, output_dim)\n        self.dropout = nn.Dropout(dropout)\n\n    def forward(self, input, hidden, cell):\n        input = input.unsqueeze(0)\n        embedded = self.dropout(self.embedding(input))\n        output, (hidden, cell) = self.rnn(embedded, (hidden, cell))\n        prediction = self.fc_out(output.squeeze(0))\n        return prediction, hidden, cell","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class Seq2Seq(nn.Module):\n    def __init__(self, encoder, decoder, device):\n        super().__init__()\n        self.encoder = encoder\n        self.decoder = decoder\n        self.device = device\n\n    def forward(self, src, trg, teacher_forcing_ratio = 0.5):\n        batch_size = trg.shape[1]\n        trg_len = trg.shape[0]\n        trg_vocab_size = self.decoder.output_dim\n\n        outputs = torch.zeros(trg_len, batch_size, trg_vocab_size).to(self.device)\n        hidden, cell = self.encoder(src)\n\n        # first input to the decoder is the <sos> tokens\n        input = trg[0,:]\n\n        for t in range(1, trg_len):\n            output, hidden, cell = self.decoder(input, hidden, cell)\n            outputs[t] = output\n            top1 = output.argmax(1) \n            input = trg[t] if (random.random() < teacher_forcing_ratio) else top1\n\n        return outputs\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from dataclasses import dataclass\nfrom typing import List\n\n@dataclass\nclass TextPair:\n    input_text: str\n    target_text: str\n\n@dataclass\nclass Vocabulary:\n    token_to_index: dict\n    index_to_token: dict\n    vocab_size: int","metadata":{"execution":{"iopub.status.busy":"2024-09-13T19:43:30.600334Z","iopub.execute_input":"2024-09-13T19:43:30.600730Z","iopub.status.idle":"2024-09-13T19:43:30.606905Z","shell.execute_reply.started":"2024-09-13T19:43:30.600692Z","shell.execute_reply":"2024-09-13T19:43:30.605965Z"},"trusted":true},"execution_count":133,"outputs":[]},{"cell_type":"code","source":"import torch\nfrom torch.utils.data import Dataset, DataLoader\n\nclass Seq2SeqDataset(Dataset):\n    def __init__(self, pairs: List[TextPair], vocab: Vocabulary):\n        self.pairs = pairs\n        self.vocab = vocab\n\n    def __len__(self):\n        return len(self.pairs)\n\n    def __getitem__(self, idx):\n        input_seq = [self.vocab.token_to_index[char] for char in self.pairs[idx].input_text]\n        target_seq = [self.vocab.token_to_index[char] for char in self.pairs[idx].target_text]\n        return torch.tensor(input_seq, dtype=torch.long), torch.tensor(target_seq, dtype=torch.long)\n\ndef create_vocab(texts: List[str]):\n    unique_chars = set(''.join(texts))\n    token_to_index = {char: i for i, char in enumerate(unique_chars, start=1)}\n    index_to_token = {i: char for char, i in token_to_index.items()}\n    vocab = Vocabulary(token_to_index, index_to_token, len(token_to_index) + 1)\n    return vocab\n\ntext_pairs = [TextPair(\"12 cats ran\", \"cats 12 ran\"), TextPair(\"7 dogs slept\", \"dogs 7 slept\")]\nvocab = create_vocab([pair.input_text + \" \" + pair.target_text for pair in text_pairs])\ndataset = Seq2SeqDataset(text_pairs, vocab)\ndataloader = DataLoader(dataset, batch_size=2, shuffle=True)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def train_model(model, dataloader, optimizer, criterion, device):\n    model.train()\n    total_loss = 0\n    for input_seq, target_seq in dataloader:\n        input_seq, target_seq = input_seq.to(device), target_seq.to(device)\n        optimizer.zero_grad()\n        output = model(input_seq, target_seq)\n        output_dim = output.shape[-1]\n        output = output[1:].view(-1, output_dim)\n        target_seq = target_seq[1:].view(-1)\n        loss = criterion(output, target_seq)\n        loss.backward()\n        optimizer.step()\n        total_loss += loss.item()\n    return total_loss / len(dataloader)\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nencoder = Encoder(vocab.vocab_size, 10, 512, 2, 0.5)\ndecoder = Decoder(vocab.vocab_size, 10, 512, 2, 0.5)\nmodel = Seq2Seq(encoder, decoder, device).to(device)\noptimizer = optim.Adam(model.parameters())\ncriterion = nn.CrossEntropyLoss()\nnum_epochs = 10\nfor epoch in range(num_epochs):\n    loss = train_model(model, dataloader, optimizer, criterion, device)\n    print(f\"Epoch {epoch+1}, Loss: {loss:.4f}\")\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def evaluate_model(model, dataloader, criterion, device):\n    model.eval()\n    total_loss = 0\n    with torch.no_grad():\n        for input_seq, target_seq in dataloader:\n            input_seq, target_seq = input_seq.to(device), target_seq.to(device)\n            output = model(input_seq, target_seq)\n            output_dim = output.shape[-1]\n            output = output[1:].view(-1, output_dim)\n            target_seq = target_seq[1:].view(-1)\n            loss = criterion(output, target_seq)\n            total_loss += loss.item()\n    return total_loss / len(dataloader)\n\ntest_loss = evaluate_model(model, dataloader, criterion, device)\nprint(f\"Test Loss: {test_loss:.4f}\")\n","metadata":{},"execution_count":null,"outputs":[]}]}